{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Advanced Feature Engineering for Economic Shock Resilience\n",
    "\n",
    "**Building Data Collection Phase**:\n",
    "- 38 high-quality countries \n",
    "- 26 World Bank indicators (100% success) \n",
    "- 1,292 observations (1990-2023) \n",
    "- 84.8% data coverage \n",
    " \n",
    "**Phase 2 Objectives**:\n",
    "1. **Economic Complexity Features**: Implement additonal indicators beyond raw metrics\n",
    "2. **Shock Resilience Metrics**: Quantify recovery patterns and vulnerability\n",
    "3. **Temporal Dynamics**: Capture economic cycles and momentum\n",
    "4. **Institutional Proxies**: Create governance and stability measures\n",
    "5. **Network Effects**: Economic integration and spillover measures\n",
    "6. **Innovation Capacity**: Technology adoption and R&D efficiency as possible measures of resilience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 2: ADVANCED FEATURE ENGINEERING\n",
      "=======================================================\n",
      "Building sophisticated economic indicators for modeling and analysis\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 15)\n",
    "\n",
    "# Load configuration\n",
    "sys.path.append(\"config\")\n",
    "from data_collection_config import *\n",
    "\n",
    "print(\"PHASE 2: ADVANCED FEATURE ENGINEERING\")\n",
    "print(\"=\" * 55)\n",
    "print(\"Building sophisticated economic indicators for modeling and analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING FINAL INTEGRATED DATASET\n",
      "=============================================\n",
      "   Dataset loaded successfully\n",
      "   Shape: (1292, 32)\n",
      "   Countries: 38\n",
      "   Years: 1990-2023\n",
      "\n",
      " DATASET OVERVIEW:\n",
      "   Total observations: 1,292\n",
      "   Variables: 32\n",
      "   Numeric variables: 29\n",
      "   Data completeness: 84.8%\n",
      "\n",
      " AVAILABLE VARIABLES:\n",
      "   Maddison variables (6): ['gdp_per_capita', 'population', 'gdp_total', 'log_gdp_per_capita', 'gdp_growth', 'population_growth_maddison']\n",
      "   World Bank variables (26): ['bank_capital_assets_ratio', 'domestic_credit_private_gdp', 'exports_gdp', 'fdi_net_inflows_gdp', 'gdp_growth_annual']...\n",
      "\n",
      " COUNTRIES (38):\n",
      "   ARG, AUS, AUT, BEL, BRA, CAN, CHE, CHL, CHN, COL, CZE, DEU, DNK, ESP, FIN, FRA, GBR, HUN, IDN, IND, IRL, ITA, JPN, KOR, MEX, MYS, NLD, NOR, NZL, PHL, POL, PRT, RUS, SWE, THA, TUR, USA, ZAF\n",
      "\n",
      " Ready to proceed with feature engineering!\n"
     ]
    }
   ],
   "source": [
    "# Load the final integrated dataset\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "print(\"LOADING FINAL INTEGRATED DATASET\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "try:\n",
    "    # Load the final dataset\n",
    "    df = pd.read_csv('data/final_integrated_dataset.csv')\n",
    "    print(f\"   Dataset loaded successfully\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Countries: {df['country_code'].nunique()}\")\n",
    "    print(f\"   Years: {df['year'].min()}-{df['year'].max()}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\n DATASET OVERVIEW:\")\n",
    "    print(f\"   Total observations: {len(df):,}\")\n",
    "    print(f\"   Variables: {len(df.columns)}\")\n",
    "    print(f\"   Numeric variables: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "    \n",
    "    # Data quality check\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    overall_completeness = df[numeric_cols].notna().mean().mean()\n",
    "    print(f\"   Data completeness: {overall_completeness:.1%}\")\n",
    "    \n",
    "    # Show columns\n",
    "    print(f\"\\n AVAILABLE VARIABLES:\")\n",
    "    maddison_vars = ['gdp_per_capita', 'population', 'gdp_total', 'log_gdp_per_capita', 'gdp_growth', 'population_growth_maddison']\n",
    "    wb_vars = [col for col in numeric_cols if col not in maddison_vars + ['year']]\n",
    "    \n",
    "    print(f\"   Maddison variables ({len(maddison_vars)}): {maddison_vars}\")\n",
    "    print(f\"   World Bank variables ({len(wb_vars)}): {wb_vars[:5]}...\")  # Show first 5\n",
    "    \n",
    "    # Countries representation\n",
    "    countries_found = sorted(df['country_code'].unique())\n",
    "    print(f\"\\n COUNTRIES ({len(countries_found)}):\")\n",
    "    print(f\"   {', '.join(countries_found)}\")\n",
    "    \n",
    "    dataset_loaded = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error loading dataset: {e}\")\n",
    "    dataset_loaded = False\n",
    "\n",
    "if dataset_loaded:\n",
    "    print(f\"\\n Ready to proceed with feature engineering!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING ECONOMIC FEATURES\n",
      "=============================================\n",
      "  Creating economic indicators...\n",
      "    Trade complexity measures\n",
      "    Financial sophistication index (3 components)\n",
      "    Innovation capacity index (4 components)\n",
      "    Fiscal capacity measures\n",
      "    Savings-investment gap\n",
      "    Economic volatility measures\n",
      "    Development level indicators\n",
      "\n",
      "   Created 12 complexity features:\n",
      "      • trade_balance\n",
      "      • trade_openness\n",
      "      • export_intensity\n",
      "      • financial_development_index\n",
      "      • innovation_capacity_index\n",
      "      • fiscal_balance\n",
      "      • fiscal_capacity\n",
      "      • savings_investment_gap\n",
      "      • gdp_per_capita_growth_volatility_3y\n",
      "      • gdp_per_capita_growth_volatility_5y\n",
      "      • gdp_per_capita_relative\n",
      "      • gdp_per_capita_trend_5y\n",
      "\n",
      " Economic complexity features complete!\n",
      "   Dataset shape: (1292, 44)\n"
     ]
    }
   ],
   "source": [
    "# Create economic indicators - Economic Complexity \n",
    "# ====================================================\n",
    "\n",
    "if dataset_loaded:\n",
    "    print(\"CREATING ECONOMIC FEATURES\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    def create_economic_complexity_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create advanced economic measures.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Input dataset\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Dataset with economic features added\n",
    "        \"\"\"\n",
    "        \n",
    "        df_complex = df.copy()\n",
    "        \n",
    "        print(\"  Creating economic indicators...\")\n",
    "        \n",
    "        # 1. Economic Diversification Index - Diversification \n",
    "        if all(col in df.columns for col in ['exports_gdp', 'imports_gdp', 'trade_gdp']):\n",
    "            df_complex['trade_balance'] = df_complex['exports_gdp'] - df_complex['imports_gdp']\n",
    "            df_complex['trade_openness'] = df_complex['trade_gdp'] / 100  # Normalize\n",
    "            df_complex['export_intensity'] = df_complex['exports_gdp'] / df_complex['trade_gdp']\n",
    "            print(\"    Trade complexity measures\")\n",
    "        \n",
    "        # 2. Financial Sophistication Index\n",
    "        financial_vars = ['domestic_credit_private_gdp', 'market_cap_gdp', 'fdi_net_inflows_gdp']\n",
    "        available_financial = [var for var in financial_vars if var in df.columns]\n",
    "        \n",
    "        if len(available_financial) >= 2:\n",
    "            # Create financial development index\n",
    "            financial_data = df_complex[available_financial].fillna(df_complex[available_financial].median())\n",
    "            scaler = StandardScaler()\n",
    "            financial_scaled = scaler.fit_transform(financial_data)\n",
    "            df_complex['financial_development_index'] = np.mean(financial_scaled, axis=1)\n",
    "            print(f\"    Financial sophistication index ({len(available_financial)} components)\")\n",
    "        \n",
    "        # 3. Innovation Capacity Index  \n",
    "        innovation_vars = ['research_development_gdp', 'patent_applications_residents', 'tertiary_education_enrollment', 'internet_users_pct']\n",
    "        available_innovation = [var for var in innovation_vars if var in df.columns]\n",
    "        \n",
    "        if len(available_innovation) >= 2:\n",
    "            innovation_data = df_complex[available_innovation].fillna(df_complex[available_innovation].median())\n",
    "            # Log transform patent applications (highly skewed)\n",
    "            if 'patent_applications_residents' in innovation_data.columns:\n",
    "                innovation_data['patent_applications_residents'] = np.log1p(innovation_data['patent_applications_residents'])\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            innovation_scaled = scaler.fit_transform(innovation_data)\n",
    "            df_complex['innovation_capacity_index'] = np.mean(innovation_scaled, axis=1)\n",
    "            print(f\"    Innovation capacity index ({len(available_innovation)} components)\")\n",
    "        \n",
    "        # 4. Fiscal Capacity Index\n",
    "        fiscal_vars = ['tax_revenue_gdp', 'government_expenditure_gdp']\n",
    "        available_fiscal = [var for var in fiscal_vars if var in df.columns]\n",
    "        \n",
    "        if len(available_fiscal) == 2:\n",
    "            df_complex['fiscal_balance'] = df_complex['tax_revenue_gdp'] - df_complex['government_expenditure_gdp']\n",
    "            df_complex['fiscal_capacity'] = (df_complex['tax_revenue_gdp'] + df_complex['government_expenditure_gdp']) / 2\n",
    "            print(\"    Fiscal capacity measures\")\n",
    "        \n",
    "        # 5. Economic Structure Sophistication\n",
    "        # Investment efficiency, how the investment affects gdp_growth\n",
    "        if all(col in df.columns for col in ['gross_investment_gdp', 'gdp_growth']):\n",
    "            df_complex['investment_efficiency'] = df_complex['gdp_growth'] / (df_complex['gross_investment_gdp'] + 0.1)  # Avoid division by zero\n",
    "            print(\"    Investment efficiency ratio\")\n",
    "        \n",
    "        # Savings-Investment gap\n",
    "        if all(col in df.columns for col in ['gross_savings_gdp', 'gross_investment_gdp']):\n",
    "            df_complex['savings_investment_gap'] = df_complex['gross_savings_gdp'] - df_complex['gross_investment_gdp']\n",
    "            print(\"    Savings-investment gap\")\n",
    "        \n",
    "        # 6. Economic Volatility Measures (rolling windows) ***\n",
    "        volatility_vars = ['gdp_growth', 'gdp_per_capita_growth']\n",
    "        for var in volatility_vars:\n",
    "            if var in df.columns:\n",
    "                df_complex[f'{var}_volatility_3y'] = (\n",
    "                    df_complex.groupby('country_code')[var]\n",
    "                    .rolling(window=3, min_periods=2)\n",
    "                    .std()\n",
    "                    .reset_index(0, drop=True)\n",
    "                )\n",
    "                df_complex[f'{var}_volatility_5y'] = (\n",
    "                    df_complex.groupby('country_code')[var]\n",
    "                    .rolling(window=5, min_periods=3)\n",
    "                    .std()\n",
    "                    .reset_index(0, drop=True)\n",
    "                )\n",
    "        \n",
    "        print(\"    Economic volatility measures\")\n",
    "        \n",
    "        # 7. Development Level Indicators\n",
    "        if 'gdp_per_capita' in df.columns:\n",
    "\n",
    "            # GDP per capita relative to global median by year\n",
    "            global_median_gdp = df_complex.groupby('year')['gdp_per_capita'].median()\n",
    "            df_complex['gdp_per_capita_relative'] = df_complex.apply(\n",
    "                lambda row: row['gdp_per_capita'] / global_median_gdp[row['year']], axis=1\n",
    "            )\n",
    "            \n",
    "            # Development trajectory (5-year growth trend)\n",
    "            df_complex['gdp_per_capita_trend_5y'] = (\n",
    "                df_complex.groupby('country_code')['gdp_per_capita']\n",
    "                .rolling(window=5, min_periods=3)\n",
    "                .apply(lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 3 else np.nan)\n",
    "                .reset_index(0, drop=True)\n",
    "            )\n",
    "            \n",
    "            print(\"    Development level indicators\")\n",
    "        \n",
    "        new_features = [col for col in df_complex.columns if col not in df.columns]\n",
    "        print(f\"\\n   Created {len(new_features)} complexity features:\")\n",
    "        for feature in new_features:\n",
    "            print(f\"      • {feature}\")\n",
    "        \n",
    "        return df_complex\n",
    "    \n",
    "    # Create economic complexity features\n",
    "    df_with_complexity = create_economic_complexity_features(df)\n",
    "    print(f\"\\n Economic complexity features complete!\")\n",
    "    print(f\"   Dataset shape: {df_with_complexity.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CREATING RESILIENCE FEATURES\n",
      "=============================================\n",
      "   Creating  shock resilience indicators...\n",
      "      Computing historical resilience context metrics...\n",
      "      Historical resilience metrics calculated\n",
      "      Creating year-varying modeling targets...\n",
      "      Year-varying modeling targets created\n",
      "      Adding temporal dynamics...\n",
      "      Temporal dynamics added\n",
      "\n",
      "   Created 16 resilience features:\n",
      "      MODELING TARGETS (5):\n",
      "         • growth_stability_target: Range[0.005, 81.532], Varies\n",
      "         • economic_performance_target: Range[0.035, 1.000], Varies\n",
      "         • relative_development_target: Range[0.108, 2.643], Varies\n",
      "         • investment_efficiency_target: Range[-0.750, 0.984], Varies\n",
      "         • composite_resilience_target: Range[0.016, 0.974], Varies\n",
      "      OTHER FEATURES (11):\n",
      "         • total_shocks_experienced\n",
      "         • avg_max_drawdown\n",
      "         • avg_recovery_time\n",
      "         • avg_recovery_strength\n",
      "         • historical_resilience_score\n",
      "         • historical_vulnerability_score\n",
      "         • gdp_growth_annual_stability_3y\n",
      "         • gdp_per_capita_growth_stability_3y\n",
      "         • pre_shock_vulnerability\n",
      "         • gdp_per_capita_momentum\n",
      "         • gdp_growth_annual_momentum\n",
      "         ... and 6 more\n",
      "\n",
      "   MODELING RECOMMENDATIONS:\n",
      "     PRIMARY TARGET: growth_stability_target\n",
      "     suitable for ML (good variance + temporal variation)\n",
      "     ALTERNATIVE 1: economic_performance_target\n",
      "     Year-wise relative performance measure\n",
      "     ALTERNATIVE 2: composite_resilience_target\n",
      "     Multi-dimensional resilience measure\n",
      "\n",
      " Shock resilience metrics complete!\n",
      "    Dataset shape: (1292, 60)\n"
     ]
    }
   ],
   "source": [
    "# Create shock resilience indicators\n",
    "# =======================================================\n",
    "\n",
    "if dataset_loaded:\n",
    "    print(\" CREATING RESILIENCE FEATURES\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    \n",
    "    def create_shock_resilience_features_FIXED(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create shock resilience and recovery metrics - CORRECTED VERSION.\n",
    "        \n",
    "        FIXES APPLIED:\n",
    "        1. Create year-varying targets instead of static country-level ones\n",
    "        2. Fix resilience variable polarity (higher = more resilient)\n",
    "        3. Add multiple target options for modeling flexibility\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe with economic indicators\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with resilience features and year-varying targets\n",
    "        \"\"\"\n",
    "        \n",
    "        df_resilient = df.copy()\n",
    "        \n",
    "        print(\"   Creating  shock resilience indicators...\")\n",
    "        \n",
    "        # =========================================================================\n",
    "        # PART 1: TRADITIONAL RESILIENCE METRICS (for context, not main targets)\n",
    "        # =========================================================================\n",
    "        \n",
    "        # we keep the original resilience calculation but as these do not consider time we wont use them as target\n",
    "        print(\"      Computing historical resilience context metrics...\")\n",
    "        \n",
    "        resilience_metrics = []\n",
    "        \n",
    "        # For each country, calculate resilience across all shocks \n",
    "        for country in df_resilient['country_code'].unique():\n",
    "            country_data = df_resilient[df_resilient['country_code'] == country].sort_values('year')\n",
    "            \n",
    "            country_resilience = {\n",
    "                'country_code': country,\n",
    "                'total_shocks_experienced': 0,\n",
    "                'avg_max_drawdown': 0,\n",
    "                'avg_recovery_time': 0,\n",
    "                'avg_recovery_strength': 0,\n",
    "                'historical_resilience_score': 0,  # Renamed for clarity\n",
    "                'historical_vulnerability_score': 0\n",
    "            }\n",
    "            \n",
    "            shock_impacts = []\n",
    "            \n",
    "            # Analyze each major shock \n",
    "            for shock_name, shock_info in MAJOR_SHOCKS.items():\n",
    "                pre_shock_years = range(max(shock_info['start'] - 3, 1990), shock_info['start'])\n",
    "                pre_shock_data = country_data[country_data['year'].isin(pre_shock_years)]\n",
    "                \n",
    "                if len(pre_shock_data) >= 2 and 'gdp_per_capita' in pre_shock_data.columns:\n",
    "                    baseline_gdp = pre_shock_data['gdp_per_capita'].mean()\n",
    "                    \n",
    "                    shock_years = range(shock_info['start'], shock_info['end'] + 1)\n",
    "                    shock_data = country_data[country_data['year'].isin(shock_years)]\n",
    "                    \n",
    "                    if len(shock_data) > 0 and baseline_gdp > 0:\n",
    "                        min_gdp_during_shock = shock_data['gdp_per_capita'].min()\n",
    "                        max_drawdown = (baseline_gdp - min_gdp_during_shock) / baseline_gdp\n",
    "                        \n",
    "                        recovery_years = range(shock_info['end'] + 1, min(shock_info['end'] + 6, 2024))\n",
    "                        recovery_data = country_data[country_data['year'].isin(recovery_years)]\n",
    "                        \n",
    "                        recovery_time = None\n",
    "                        recovery_strength = 0\n",
    "                        \n",
    "                        if len(recovery_data) > 0:\n",
    "                            for _, row in recovery_data.iterrows():\n",
    "                                if row['gdp_per_capita'] >= baseline_gdp:\n",
    "                                    recovery_time = row['year'] - shock_info['end']\n",
    "                                    break\n",
    "                            \n",
    "                            if 'gdp_growth' in recovery_data.columns:\n",
    "                                recovery_strength = recovery_data['gdp_growth'].mean()\n",
    "                        \n",
    "                        shock_impacts.append({\n",
    "                            'shock': shock_name,\n",
    "                            'max_drawdown': max_drawdown,\n",
    "                            'recovery_time': recovery_time,\n",
    "                            'recovery_strength': recovery_strength,\n",
    "                            'baseline_gdp': baseline_gdp\n",
    "                        })\n",
    "            \n",
    "            # Aggregate resilience metrics \n",
    "            if shock_impacts:\n",
    "                country_resilience['total_shocks_experienced'] = len(shock_impacts)\n",
    "                country_resilience['avg_max_drawdown'] = np.mean([s['max_drawdown'] for s in shock_impacts])\n",
    "                \n",
    "                recovery_times = [s['recovery_time'] for s in shock_impacts if s['recovery_time'] is not None]\n",
    "                if recovery_times:\n",
    "                    country_resilience['avg_recovery_time'] = np.mean(recovery_times)\n",
    "                \n",
    "                recovery_strengths = [s['recovery_strength'] for s in shock_impacts if not np.isnan(s['recovery_strength'])]\n",
    "                if recovery_strengths:\n",
    "                    country_resilience['avg_recovery_strength'] = np.mean(recovery_strengths)\n",
    "\n",
    "                \n",
    "                # FIXED RESILIENCE SCORE CALCULATION - CORRECT POLARITY\n",
    "                resilience_components = []\n",
    "                if country_resilience['avg_max_drawdown'] != 0:\n",
    "                    # Lower drawdown = higher resilience (positive)\n",
    "                    resilience_components.append(1 - country_resilience['avg_max_drawdown'])  # FIXED: make sure is positive\n",
    "                if country_resilience['avg_recovery_time'] > 0:\n",
    "                    # Faster recovery = higher resilience (invert and normalize)\n",
    "                    resilience_components.append(1 - min(country_resilience['avg_recovery_time'] / 10, 1))  # FIXED: Proper normalization\n",
    "                if country_resilience['avg_recovery_strength'] != 0:\n",
    "                    # Stronger recovery = higher resilience (normalize to 0-1)\n",
    "                    resilience_components.append(min(max(country_resilience['avg_recovery_strength'] / 5, 0), 1))  # FIXED: Proper scaling\n",
    "                \n",
    "                if resilience_components:\n",
    "                    country_resilience['historical_resilience_score'] = np.mean(resilience_components)\n",
    "                    country_resilience['historical_vulnerability_score'] = 1 - country_resilience['historical_resilience_score']\n",
    "            \n",
    "            resilience_metrics.append(country_resilience)\n",
    "        \n",
    "        # Convert to DataFrame and merge (same as before)\n",
    "        resilience_df = pd.DataFrame(resilience_metrics)\n",
    "        df_resilient = df_resilient.merge(resilience_df, on='country_code', how='left')\n",
    "        \n",
    "        print(f\"      Historical resilience metrics calculated\")\n",
    "        \n",
    "        # =========================================================================\n",
    "        # PART 2: YEAR-VARYING MODELING TARGETS (THE KEY FIX!)\n",
    "        # This is probably the most important piece of this project\n",
    "        # =========================================================================\n",
    "        \n",
    "        print(\"      Creating year-varying modeling targets...\")\n",
    "        \n",
    "        # TARGET 1: Economic Growth Stability (varies by year)\n",
    "        # Inverse coefficient of variation of GDP growth (more stable = higher score)\n",
    "        # Shows the inverse link between volatility and long‑run prosperity—direct support for treating stability as resilience\n",
    "\n",
    "        df_resilient['growth_stability_target'] = (\n",
    "            df_resilient.groupby('country_code')['gdp_growth_annual']\n",
    "            .rolling(window=3, min_periods=2)\n",
    "            .apply(lambda x: 1 / (np.std(x) / (np.abs(np.mean(x)) + 0.01) + 0.01))\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # TARGET 2: Economic Performance Index (varies by year) \n",
    "        # Multi-dimensional performance relative to peers in same year\n",
    "\n",
    "        performance_indicators = [\n",
    "            'gdp_per_capita_growth', 'gross_investment_gdp', 'gross_savings_gdp'\n",
    "        ]\n",
    "        \n",
    "        available_indicators = [col for col in performance_indicators if col in df_resilient.columns]\n",
    "        \n",
    "        if len(available_indicators) >= 2:\n",
    "            performance_components = []\n",
    "            \n",
    "            for indicator in available_indicators:\n",
    "                # Year-wise percentile ranking (0-1 scale)\n",
    "                yearly_percentile = (\n",
    "                    df_resilient.groupby('year')[indicator]\n",
    "                    .transform(lambda x: x.rank(pct=True))\n",
    "                )\n",
    "                performance_components.append(yearly_percentile)\n",
    "            \n",
    "            df_resilient['economic_performance_target'] = np.mean(performance_components, axis=0)\n",
    "        \n",
    "        # TARGET 3: Relative Development Position (varies by year)\n",
    "        # Position relative to global development in each year\n",
    "\n",
    "        if 'gdp_per_capita' in df_resilient.columns:\n",
    "            global_median_gdp = df_resilient.groupby('year')['gdp_per_capita'].median()\n",
    "            df_resilient['relative_development_target'] = df_resilient.apply(\n",
    "                lambda row: row['gdp_per_capita'] / global_median_gdp[row['year']] \n",
    "                if row['year'] in global_median_gdp.index else np.nan, axis=1\n",
    "            )\n",
    "        \n",
    "        # TARGET 4: Investment Efficiency (varies by year)\n",
    "        # GDP growth per unit of investment\n",
    "        if all(col in df_resilient.columns for col in ['gdp_growth_annual', 'gross_investment_gdp']):\n",
    "            df_resilient['investment_efficiency_target'] = (\n",
    "                (df_resilient['gdp_growth_annual'] + 2) / (df_resilient['gross_investment_gdp'] + 1)\n",
    "            )  # Add constants to avoid negatives/zeros\n",
    "        \n",
    "        # TARGET 5: Economic Resilience Composite (varies by year)\n",
    "        # Combines stability, performance, and efficiency\n",
    "\n",
    "        target_components = []\n",
    "        \n",
    "        if 'growth_stability_target' in df_resilient.columns:\n",
    "            # Normalize to 0-1 scale\n",
    "            stability_norm = df_resilient['growth_stability_target'] / df_resilient['growth_stability_target'].quantile(0.95)\n",
    "            stability_norm = stability_norm.clip(0, 1)\n",
    "            target_components.append(stability_norm)\n",
    "        \n",
    "        if 'economic_performance_target' in df_resilient.columns:\n",
    "            target_components.append(df_resilient['economic_performance_target'])\n",
    "        \n",
    "        if 'investment_efficiency_target' in df_resilient.columns:\n",
    "            # Normalize to 0-1 scale\n",
    "            efficiency_norm = df_resilient['investment_efficiency_target'] / df_resilient['investment_efficiency_target'].quantile(0.95)\n",
    "            efficiency_norm = efficiency_norm.clip(0, 1)\n",
    "            target_components.append(efficiency_norm)\n",
    "        \n",
    "        if target_components:\n",
    "            df_resilient['composite_resilience_target'] = np.mean(target_components, axis=0)\n",
    "        \n",
    "        print(f\"      Year-varying modeling targets created\")\n",
    "        \n",
    "        # =========================================================================\n",
    "        # PART 3: TEMPORAL DYNAMICS \n",
    "        # =========================================================================\n",
    "        \n",
    "        print(\"      Adding temporal dynamics...\")\n",
    "        \n",
    "        # Economic stability indicators \n",
    "        stability_vars = ['gdp_growth_annual', 'gdp_per_capita_growth']\n",
    "        for var in stability_vars:\n",
    "            if var in df_resilient.columns:\n",
    "                # 3-year coefficient of variation \n",
    "                df_resilient[f'{var}_stability_3y'] = (\n",
    "                    df_resilient.groupby('country_code')[var]\n",
    "                    .rolling(window=3, min_periods=2)\n",
    "                    .apply(lambda x: np.std(x) / (np.abs(np.mean(x)) + 0.01))\n",
    "                    .reset_index(0, drop=True)\n",
    "                )\n",
    "        \n",
    "        # Pre-shock vulnerability \n",
    "        df_resilient['pre_shock_vulnerability'] = 0\n",
    "        \n",
    "        for idx, row in df_resilient.iterrows():\n",
    "            year = row['year']\n",
    "            vulnerability_factors = []\n",
    "            \n",
    "            for shock_name, shock_info in MAJOR_SHOCKS.items():\n",
    "                if year == shock_info['start'] - 1:  # Year before shock\n",
    "                    if not pd.isna(row.get('government_debt_gdp', np.nan)):\n",
    "                        if row['government_debt_gdp'] > 60:\n",
    "                            vulnerability_factors.append(1)\n",
    "                    \n",
    "                    if not pd.isna(row.get('gross_savings_gdp', np.nan)):\n",
    "                        if row['gross_savings_gdp'] < 15:\n",
    "                            vulnerability_factors.append(1)\n",
    "                    \n",
    "                    if not pd.isna(row.get('trade_gdp', np.nan)):\n",
    "                        if row['trade_gdp'] > 100:\n",
    "                            vulnerability_factors.append(0.5)\n",
    "            \n",
    "            if vulnerability_factors:\n",
    "                df_resilient.at[idx, 'pre_shock_vulnerability'] = np.mean(vulnerability_factors)\n",
    "        \n",
    "        # Recovery momentum (same as before)\n",
    "        for var in ['gdp_per_capita', 'gdp_growth_annual']:\n",
    "            if var in df_resilient.columns:\n",
    "                df_resilient[f'{var}_momentum'] = (\n",
    "                    df_resilient.groupby('country_code')[var]\n",
    "                    .diff(2)\n",
    "                )\n",
    "        \n",
    "        print(f\"      Temporal dynamics added\")\n",
    "        \n",
    "        # =========================================================================\n",
    "        # PART 4: SUMMARY AND RECOMMENDATIONS\n",
    "        # =========================================================================\n",
    "        \n",
    "        new_features = [col for col in df_resilient.columns if col not in df.columns]\n",
    "        \n",
    "        print(f\"\\n   Created {len(new_features)} resilience features:\")\n",
    "        \n",
    "        # Separate targets from other features\n",
    "        target_features = [f for f in new_features if 'target' in f]\n",
    "        other_features = [f for f in new_features if 'target' not in f]\n",
    "        \n",
    "        print(f\"      MODELING TARGETS ({len(target_features)}):\")\n",
    "        for feature in target_features:\n",
    "            target_data = df_resilient[feature].dropna()\n",
    "            if len(target_data) > 0:\n",
    "                # Check temporal variation\n",
    "                temporal_var = (\n",
    "                    df_resilient.groupby('country_code')[feature]\n",
    "                    .apply(lambda x: x.std() > 0.01 if len(x) > 1 else False)\n",
    "                    .mean()\n",
    "                )\n",
    "                temp_status = \"Varies\" if temporal_var > 0.5 else \"⚠️ Static\"\n",
    "                print(f\"         • {feature}: Range[{target_data.min():.3f}, {target_data.max():.3f}], {temp_status}\")\n",
    "        \n",
    "        print(f\"      OTHER FEATURES ({len(other_features)}):\")\n",
    "        for feature in other_features[:11]:  # Show first 5\n",
    "            print(f\"         • {feature}\")\n",
    "        if len(other_features) > 5:\n",
    "            print(f\"         ... and {len(other_features) - 5} more\")\n",
    "        \n",
    "        # RECOMMENDATIONS\n",
    "        print(f\"\\n   MODELING RECOMMENDATIONS:\")\n",
    "        print(f\"     PRIMARY TARGET: growth_stability_target\")\n",
    "        print(f\"     suitable for ML (good variance + temporal variation)\")\n",
    "        print(f\"     ALTERNATIVE 1: economic_performance_target\") \n",
    "        print(f\"     Year-wise relative performance measure\")\n",
    "        print(f\"     ALTERNATIVE 2: composite_resilience_target\")\n",
    "        print(f\"     Multi-dimensional resilience measure\")\n",
    "        \n",
    "        return df_resilient\n",
    "    \n",
    "# Create resilience features\n",
    "df_with_resilience = create_shock_resilience_features_FIXED(df_with_complexity)\n",
    "print(f\"\\n Shock resilience metrics complete!\")\n",
    "print(f\"    Dataset shape: {df_with_resilience.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CREATING TEMPORAL & CYCLICAL FEATURES\n",
      "=============================================\n",
      "   Creating temporal dynamics...\n",
      "      Trend and momentum features\n",
      "      Lag features\n",
      "      Convergence indicators\n",
      "      Crisis proximity features\n",
      "      Period effect indicators\n",
      "\n",
      "   Created 18 temporal features:\n",
      "      • gdp_per_capita_acceleration\n",
      "      • trade_gdp_trend_5y\n",
      "      • trade_gdp_acceleration\n",
      "      • gross_investment_gdp_trend_5y\n",
      "      • gross_investment_gdp_acceleration\n",
      "      • gross_investment_gdp_lag1\n",
      "      • gross_investment_gdp_lag2\n",
      "      • unemployment_total_lag1\n",
      "      ... and 10 more\n",
      "\n",
      " Temporal features complete!\n",
      "   Dataset shape: (1292, 78)\n"
     ]
    }
   ],
   "source": [
    "# Create temporal dynamics and cyclical features\n",
    "\n",
    "if dataset_loaded:\n",
    "    print(\" CREATING TEMPORAL & CYCLICAL FEATURES\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    def create_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create sophisticated temporal and cyclical economic features.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_temporal = df.copy().sort_values(['country_code', 'year'])\n",
    "        \n",
    "        print(\"   Creating temporal dynamics...\")\n",
    "        \n",
    "        # 1. Economic cycle indicators\n",
    "        if 'gdp_growth' in df_temporal.columns:\n",
    "            # Detect economic cycles using rolling windows\n",
    "            df_temporal['gdp_growth_ma_3y'] = (\n",
    "                df_temporal.groupby('country_code')['gdp_growth']\n",
    "                .rolling(window=3, center=True, min_periods=2)\n",
    "                .mean()\n",
    "                .reset_index(0, drop=True)\n",
    "            )\n",
    "            \n",
    "            df_temporal['gdp_growth_ma_5y'] = (\n",
    "                df_temporal.groupby('country_code')['gdp_growth']\n",
    "                .rolling(window=5, center=True, min_periods=3)\n",
    "                .mean()\n",
    "                .reset_index(0, drop=True)\n",
    "            )\n",
    "            \n",
    "            # Cycle position (above/below trend)\n",
    "            df_temporal['gdp_cycle_position'] = (\n",
    "                df_temporal['gdp_growth'] - df_temporal['gdp_growth_ma_5y']\n",
    "            )\n",
    "            \n",
    "            # Expansion/contraction phases\n",
    "            df_temporal['in_expansion'] = (df_temporal['gdp_growth'] > 0).astype(int)\n",
    "            df_temporal['in_recession'] = (df_temporal['gdp_growth'] < -1).astype(int)  # Technical recession threshold\n",
    "            \n",
    "            print(\"      Economic cycle indicators\")\n",
    "        \n",
    "        # 2. Trend and momentum features\n",
    "        trend_vars = ['gdp_per_capita', 'trade_gdp', 'gross_investment_gdp']\n",
    "        \n",
    "        for var in trend_vars:\n",
    "            if var in df_temporal.columns:\n",
    "                # Linear trend over 5 years\n",
    "                df_temporal[f'{var}_trend_5y'] = (\n",
    "                    df_temporal.groupby('country_code')[var]\n",
    "                    .rolling(window=5, min_periods=3)\n",
    "                    .apply(lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 3 else np.nan)\n",
    "                    .reset_index(0, drop=True)\n",
    "                )\n",
    "                \n",
    "                # Acceleration (second derivative)\n",
    "                df_temporal[f'{var}_acceleration'] = (\n",
    "                    df_temporal.groupby('country_code')[f'{var}_trend_5y']\n",
    "                    .diff()\n",
    "                )\n",
    "        \n",
    "        print(\"      Trend and momentum features\")\n",
    "        \n",
    "        # 3. Lag features (economic conditions persistence)\n",
    "        lag_vars = ['gdp_growth', 'gross_investment_gdp', 'unemployment_total']\n",
    "        \n",
    "        for var in lag_vars:\n",
    "            if var in df_temporal.columns:\n",
    "                # 1-year and 2-year lags\n",
    "                df_temporal[f'{var}_lag1'] = (\n",
    "                    df_temporal.groupby('country_code')[var].shift(1)\n",
    "                )\n",
    "                df_temporal[f'{var}_lag2'] = (\n",
    "                    df_temporal.groupby('country_code')[var].shift(2)\n",
    "                )\n",
    "        \n",
    "        print(\"      Lag features\")\n",
    "        \n",
    "        # 4. Convergence/divergence indicators\n",
    "        if 'gdp_per_capita' in df_temporal.columns:\n",
    "            # Global convergence (catching up to rich countries)\n",
    "            annual_top_decile = (\n",
    "                df_temporal.groupby('year')['gdp_per_capita']\n",
    "                .quantile(0.9)\n",
    "                .to_dict()\n",
    "            )\n",
    "            \n",
    "            df_temporal['convergence_gap'] = df_temporal.apply(\n",
    "                lambda row: annual_top_decile[row['year']] / (row['gdp_per_capita'] + 1), axis=1\n",
    "            )\n",
    "            \n",
    "            # Convergence speed (how fast the gap is closing)\n",
    "            df_temporal['convergence_speed'] = (\n",
    "                df_temporal.groupby('country_code')['convergence_gap']\n",
    "                .diff(-1)  # Negative diff (gap should decrease)\n",
    "            )\n",
    "            \n",
    "            print(\"      Convergence indicators\")\n",
    "        \n",
    "        # 5. Crisis proximity features\n",
    "        df_temporal['years_to_next_shock'] = np.inf\n",
    "        df_temporal['years_since_last_shock'] = np.inf\n",
    "        \n",
    "        for idx, row in df_temporal.iterrows():\n",
    "            year = row['year']\n",
    "            \n",
    "            # Find next shock\n",
    "            next_shock_years = []\n",
    "            last_shock_years = []\n",
    "            \n",
    "            for shock_name, shock_info in MAJOR_SHOCKS.items():\n",
    "                if shock_info['start'] > year:\n",
    "                    next_shock_years.append(shock_info['start'] - year)\n",
    "                if shock_info['end'] < year:\n",
    "                    last_shock_years.append(year - shock_info['end'])\n",
    "            \n",
    "            if next_shock_years:\n",
    "                df_temporal.at[idx, 'years_to_next_shock'] = min(next_shock_years)\n",
    "            if last_shock_years:\n",
    "                df_temporal.at[idx, 'years_since_last_shock'] = min(last_shock_years)\n",
    "        \n",
    "        # Cap at reasonable values\n",
    "        df_temporal['years_to_next_shock'] = np.clip(df_temporal['years_to_next_shock'], 0, 10)\n",
    "        df_temporal['years_since_last_shock'] = np.clip(df_temporal['years_since_last_shock'], 0, 10)\n",
    "        \n",
    "        print(\"      Crisis proximity features\")\n",
    "        \n",
    "        # 6. Decade and period effects\n",
    "        df_temporal['decade'] = (df_temporal['year'] // 10) * 10\n",
    "        df_temporal['post_cold_war'] = (df_temporal['year'] >= 1991).astype(int)\n",
    "        df_temporal['globalization_era'] = (df_temporal['year'] >= 1995).astype(int)\n",
    "        df_temporal['post_2008_crisis'] = (df_temporal['year'] >= 2009).astype(int)\n",
    "        df_temporal['post_covid'] = (df_temporal['year'] >= 2021).astype(int)\n",
    "        \n",
    "        print(\"      Period effect indicators\")\n",
    "        \n",
    "        new_temporal_features = [col for col in df_temporal.columns if col not in df.columns]\n",
    "        print(f\"\\n   Created {len(new_temporal_features)} temporal features:\")\n",
    "        for feature in new_temporal_features[:8]:  # Show first 8\n",
    "            print(f\"      • {feature}\")\n",
    "        if len(new_temporal_features) > 8:\n",
    "            print(f\"      ... and {len(new_temporal_features) - 8} more\")\n",
    "        \n",
    "        return df_temporal\n",
    "    \n",
    "    # Create temporal features\n",
    "    df_with_temporal = create_temporal_features(df_with_resilience)\n",
    "    print(f\"\\n Temporal features complete!\")\n",
    "    print(f\"   Dataset shape: {df_with_temporal.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FEATURE ENGINEERING SUMMARY & SAVE\n",
      "=============================================\n",
      "   FEATURE ENGINEERING IMPACT:\n",
      "   Original dataset: (1292, 32)\n",
      "   Final dataset: (1292, 78)\n",
      "   Features added: 46\n",
      "   Feature expansion: 143.8%\n",
      "   DATA QUALITY:\n",
      "   Total numeric features: 75\n",
      "   Overall completeness: 91.7%\n",
      "\n",
      "  FEATURE CATEGORIES:\n",
      "   Original: 32 features\n",
      "   Complexity: 7 features\n",
      "   Resilience: 7 features\n",
      "   Temporal: 9 features\n",
      "   Volatility: 5 features\n",
      "   Period_Effects: 12 features\n",
      "\n",
      " SAVED: data/engineered_features_dataset.csv\n",
      " SAVED: data/feature_catalog.csv\n",
      "\n",
      " TOP 10 FEATURES BY COMPLETENESS:\n",
      "   year: 100.0%\n",
      "   historical_vulnerability_score: 100.0%\n",
      "   innovation_capacity_index: 100.0%\n",
      "   total_shocks_experienced: 100.0%\n",
      "   avg_max_drawdown: 100.0%\n",
      "   avg_recovery_time: 100.0%\n",
      "   avg_recovery_strength: 100.0%\n",
      "   historical_resilience_score: 100.0%\n",
      "   pre_shock_vulnerability: 100.0%\n",
      "   urban_population_pct: 100.0%\n",
      "\n",
      "  FEATURES NEEDING ATTENTION (high missing data):\n",
      "   bank_capital_assets_ratio: 43.6%\n",
      "   private_investment_gdp: 24.8%\n",
      "   domestic_credit_private_gdp: 23.8%\n",
      "\n",
      "  READY FOR NEXT PHASES : EDA & ADVANCED MODELING!\n",
      "   Engineered dataset ready for ML pipeline\n",
      "   75 features for sophisticated modeling\n",
      "   Rich feature set across all economic dimensions\n",
      "\n",
      "  CORRECTED TARGET VARIABLES FOR MODELING:\n",
      "   PRIMARY: growth_stability_target\n",
      "      → Year-varying economic stability measure\n",
      "      → Higher values = more stable growth patterns\n",
      "   SECONDARY: economic_performance_target\n",
      "      → Relative performance vs. global peers each year\n",
      "   COMPOSITE: composite_resilience_target\n",
      "      → Multi-dimensional resilience score\n",
      "   AVOID: resilience_score, vulnerability_score\n",
      "      → Static country-level metrics (not suitable for ML)\n",
      "\n",
      " PHASE 2 COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final feature engineering summary and save\n",
    "if dataset_loaded:\n",
    "    print(\" FEATURE ENGINEERING SUMMARY & SAVE\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Calculate feature engineering impact\n",
    "    original_shape = df.shape\n",
    "    final_shape = df_with_temporal.shape\n",
    "    features_added = final_shape[1] - original_shape[1]\n",
    "    \n",
    "    print(f\"   FEATURE ENGINEERING IMPACT:\")\n",
    "    print(f\"   Original dataset: {original_shape}\")\n",
    "    print(f\"   Final dataset: {final_shape}\")\n",
    "    print(f\"   Features added: {features_added}\")\n",
    "    print(f\"   Feature expansion: {features_added/original_shape[1]:.1%}\")\n",
    "    \n",
    "    # Data quality assessment\n",
    "    numeric_cols = df_with_temporal.select_dtypes(include=[np.number]).columns\n",
    "    final_completeness = df_with_temporal[numeric_cols].notna().mean().mean()\n",
    "    \n",
    "    print(f\"   DATA QUALITY:\")\n",
    "    print(f\"   Total numeric features: {len(numeric_cols)}\")\n",
    "    print(f\"   Overall completeness: {final_completeness:.1%}\")\n",
    "    \n",
    "    # Feature categories summary\n",
    "    feature_categories = {\n",
    "        'Original': [col for col in df.columns if col in df_with_temporal.columns],\n",
    "        'Complexity': [col for col in df_with_temporal.columns if any(x in col for x in ['_index', 'efficiency', 'balance', 'relative'])],\n",
    "        'Resilience': [col for col in df_with_temporal.columns if any(x in col for x in ['resilience', 'vulnerability', 'drawdown', 'recovery'])],\n",
    "        'Temporal': [col for col in df_with_temporal.columns if any(x in col for x in ['trend', 'lag', 'momentum', 'cycle', 'ma_'])],\n",
    "        'Volatility': [col for col in df_with_temporal.columns if 'volatility' in col or 'stability' in col],\n",
    "        'Period_Effects': [col for col in df_with_temporal.columns if any(x in col for x in ['post_', 'era', 'decade', 'years_'])]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  FEATURE CATEGORIES:\")\n",
    "    for category, features in feature_categories.items():\n",
    "        print(f\"   {category}: {len(features)} features\")\n",
    "    \n",
    "    # Save the engineered dataset\n",
    "    df_with_temporal.to_csv('data/engineered_features_dataset.csv', index=False)\n",
    "    print(f\"\\n SAVED: data/engineered_features_dataset.csv\")\n",
    "    \n",
    "    # Create feature catalog\n",
    "    feature_catalog = []\n",
    "    for category, features in feature_categories.items():\n",
    "        for feature in features:\n",
    "            feature_catalog.append({\n",
    "                'feature_name': feature,\n",
    "                'category': category,\n",
    "                'data_type': str(df_with_temporal[feature].dtype),\n",
    "                'missing_pct': df_with_temporal[feature].isna().mean() * 100,\n",
    "                'description': f\"{category} feature: {feature}\"\n",
    "            })\n",
    "    \n",
    "    feature_catalog_df = pd.DataFrame(feature_catalog)\n",
    "    feature_catalog_df.to_csv('data/feature_catalog.csv', index=False)\n",
    "    print(f\" SAVED: data/feature_catalog.csv\")\n",
    "    \n",
    "    # Show top features by completeness\n",
    "    completeness_by_feature = df_with_temporal[numeric_cols].notna().mean().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\n TOP 10 FEATURES BY COMPLETENESS:\")\n",
    "    for feature, completeness in completeness_by_feature.head(10).items():\n",
    "        print(f\"   {feature}: {completeness:.1%}\")\n",
    "    \n",
    "    print(f\"\\n  FEATURES NEEDING ATTENTION (high missing data):\")\n",
    "    low_completeness = completeness_by_feature[completeness_by_feature < 0.5]\n",
    "    if len(low_completeness) > 0:\n",
    "        for feature, completeness in low_completeness.head(5).items():\n",
    "            print(f\"   {feature}: {completeness:.1%}\")\n",
    "    else:\n",
    "        print(\"   None - all features have >50% completeness!\")\n",
    "    \n",
    "    print(f\"\\n  READY FOR NEXT PHASES : EDA & ADVANCED MODELING!\")\n",
    "    print(f\"   Engineered dataset ready for ML pipeline\")\n",
    "    print(f\"   {len(numeric_cols)} features for sophisticated modeling\")\n",
    "    print(f\"   Rich feature set across all economic dimensions\")\n",
    "    print(f\"\\n  CORRECTED TARGET VARIABLES FOR MODELING:\")\n",
    "    print(f\"   PRIMARY: growth_stability_target\")\n",
    "    print(f\"      → Year-varying economic stability measure\")\n",
    "    print(f\"      → Higher values = more stable growth patterns\")\n",
    "    print(f\"   SECONDARY: economic_performance_target\")\n",
    "    print(f\"      → Relative performance vs. global peers each year\")\n",
    "    print(f\"   COMPOSITE: composite_resilience_target\")\n",
    "    print(f\"      → Multi-dimensional resilience score\")\n",
    "    print(f\"   AVOID: resilience_score, vulnerability_score\")\n",
    "    print(f\"      → Static country-level metrics (not suitable for ML)\")\n",
    "    engineering_success = True\n",
    "\n",
    "else:\n",
    "    engineering_success = False\n",
    "    print(\" Feature engineering skipped due to data loading issues\")\n",
    "\n",
    "print(f\"\\n PHASE 2 COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TARGET VARIABLE ANALYSIS\n",
      "==================================================\n",
      " AVAILABLE TARGETS (5):\n",
      "\n",
      "   GROWTH_STABILITY_TARGET:\n",
      "      Statistics: μ=4.658, σ=7.407, range=[0.005, 81.532]\n",
      "      Temporal variation: 100.0% of countries show time variation\n",
      "      Cross-country CV: 0.649\n",
      "      Modeling suitability:  EXCELLENT for ML modeling\n",
      "\n",
      "   ECONOMIC_PERFORMANCE_TARGET:\n",
      "      Statistics: μ=0.513, σ=0.226, range=[0.035, 1.000]\n",
      "      Temporal variation: 100.0% of countries show time variation\n",
      "      Cross-country CV: 0.347\n",
      "      Modeling suitability:  EXCELLENT for ML modeling\n",
      "\n",
      "   RELATIVE_DEVELOPMENT_TARGET:\n",
      "      Statistics: μ=0.961, σ=0.521, range=[0.108, 2.643]\n",
      "      Temporal variation: 100.0% of countries show time variation\n",
      "      Cross-country CV: 0.532\n",
      "      Modeling suitability:  EXCELLENT for ML modeling\n",
      "\n",
      "   INVESTMENT_EFFICIENCY_TARGET:\n",
      "      Statistics: μ=0.191, σ=0.144, range=[-0.750, 0.984]\n",
      "      Temporal variation: 100.0% of countries show time variation\n",
      "      Cross-country CV: 0.245\n",
      "      Modeling suitability:  EXCELLENT for ML modeling\n",
      "\n",
      "   COMPOSITE_RESILIENCE_TARGET:\n",
      "      Statistics: μ=0.422, σ=0.189, range=[0.016, 0.974]\n",
      "      Temporal variation: 100.0% of countries show time variation\n",
      "      Cross-country CV: 0.251\n",
      "      Modeling suitability:  EXCELLENT for ML modeling\n",
      "\n",
      " TARGET ANALYSIS COMPLETE!\n",
      "   Use 'growth_stability_target' as primary y variable\n",
      "   Expect positive R² scores with proper temporal patterns\n"
     ]
    }
   ],
   "source": [
    "# Target Variable Analysis - Verify the fixes worked\n",
    "\n",
    "if dataset_loaded and engineering_success:\n",
    "    print(\" TARGET VARIABLE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    target_vars = [col for col in df_with_temporal.columns if 'target' in col]\n",
    "    \n",
    "    print(f\" AVAILABLE TARGETS ({len(target_vars)}):\")\n",
    "    \n",
    "    for target in target_vars:\n",
    "        target_data = df_with_temporal[target].dropna()\n",
    "        \n",
    "        if len(target_data) > 0:\n",
    "            # Basic statistics\n",
    "            stats = {\n",
    "                'count': len(target_data),\n",
    "                'mean': target_data.mean(),\n",
    "                'std': target_data.std(),\n",
    "                'min': target_data.min(),\n",
    "                'max': target_data.max(),\n",
    "                'range': target_data.max() - target_data.min()\n",
    "            }\n",
    "            \n",
    "            # Temporal variation check\n",
    "            temporal_variation = (\n",
    "                df_with_temporal.groupby('country_code')[target]\n",
    "                .apply(lambda x: x.std() > 0.01 if len(x) > 1 else False)\n",
    "                .mean()\n",
    "            )\n",
    "            \n",
    "            # Cross-country variation check  \n",
    "            country_means = df_with_temporal.groupby('country_code')[target].mean()\n",
    "            cross_country_var = country_means.std() / (country_means.mean() + 0.01)\n",
    "            \n",
    "            print(f\"\\n   {target.upper()}:\")\n",
    "            print(f\"      Statistics: μ={stats['mean']:.3f}, σ={stats['std']:.3f}, range=[{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
    "            print(f\"      Temporal variation: {temporal_variation:.1%} of countries show time variation\")\n",
    "            print(f\"      Cross-country CV: {cross_country_var:.3f}\")\n",
    "            \n",
    "            # Suitability assessment\n",
    "            if temporal_variation > 0.3 and cross_country_var > 0.1 and stats['range'] > 0.1:\n",
    "                suitability = \" EXCELLENT for ML modeling\"\n",
    "            elif temporal_variation > 0.2 and cross_country_var > 0.05:\n",
    "                suitability = \" GOOD for ML modeling\"  \n",
    "            else:\n",
    "                suitability = \" NOT SUITABLE for ML modeling\"\n",
    "            \n",
    "            print(f\"      Modeling suitability: {suitability}\")\n",
    "    \n",
    "    print(f\"\\n TARGET ANALYSIS COMPLETE!\")\n",
    "    print(f\"   Use 'growth_stability_target' as primary y variable\")\n",
    "    print(f\"   Expect positive R² scores with proper temporal patterns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
